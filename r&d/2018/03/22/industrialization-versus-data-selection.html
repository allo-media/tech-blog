<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <!-- Begin Jekyll SEO tag v2.4.0 -->
<title>Brace yourself data selection, industrialization is coming! | Allo-Media Technical Blog</title>
<meta name="generator" content="Jekyll v3.7.3" />
<meta property="og:title" content="Brace yourself data selection, industrialization is coming!" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Brace yourself data selection, industrialization is coming!" />
<meta property="og:description" content="Brace yourself data selection, industrialization is coming!" />
<link rel="canonical" href="/r&d/2018/03/22/industrialization-versus-data-selection.html" />
<meta property="og:url" content="/r&d/2018/03/22/industrialization-versus-data-selection.html" />
<meta property="og:site_name" content="Allo-Media Technical Blog" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2018-03-22T09:00:00+01:00" />
<script type="application/ld+json">
{"description":"Brace yourself data selection, industrialization is coming!","mainEntityOfPage":{"@type":"WebPage","@id":"/r&d/2018/03/22/industrialization-versus-data-selection.html"},"@type":"BlogPosting","url":"/r&d/2018/03/22/industrialization-versus-data-selection.html","headline":"Brace yourself data selection, industrialization is coming!","dateModified":"2018-03-22T09:00:00+01:00","datePublished":"2018-03-22T09:00:00+01:00","@context":"http://schema.org"}</script>
<!-- End Jekyll SEO tag -->

  <link rel="stylesheet" href="/assets/main.css">
  <link href="https://fonts.googleapis.com/css?family=Lato:300,300i,400,400i,700,700i" rel="stylesheet">
  <link rel="alternate" type="application/rss+xml" title="Allo-Media Technical Blog" href="/feed.xml">
  
</head>
<body><header class="site-header" role="banner">

  <div class="wrapper">
    
    
    
    <a class="site-title" rel="author" href="/"><img src="/assets/logo.svg" /><span>Tech Blog</span></a>

    
      <nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path fill="#424242" d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.031C17.335,0,18,0.665,18,1.484L18,1.484z"/>
              <path fill="#424242" d="M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0c0-0.82,0.665-1.484,1.484-1.484 h15.031C17.335,6.031,18,6.696,18,7.516L18,7.516z"/>
              <path fill="#424242" d="M18,13.516C18,14.335,17.335,15,16.516,15H1.484C0.665,15,0,14.335,0,13.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.031C17.335,12.031,18,12.696,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger">
          
            
            
          
            
            
            <a class="page-link" href="/about/">About</a>
            
          
            
            
          
            
            
          
            
            
          
            
            
          
            
            
          
            
            
          
            
            
          
            
            
          
            
            
          
            
            
          
            
            
          
            
            
          
            
            
          
            
            
          
            
            
          
        </div>
      </nav>
    
  </div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Brace yourself data selection, industrialization is coming!</h1>

    <span>
      
        
        <a href="/tag/ASR"><code class="highligher-rouge"><nobr>ASR</nobr></code></a>
      
        
        <a href="/tag/data"><code class="highligher-rouge"><nobr>data</nobr></code></a>
      
        
        <a href="/tag/selection"><code class="highligher-rouge"><nobr>selection</nobr></code></a>
      
    </span>

    <p class="post-meta">
      <time class="dt-published" datetime="2018-03-22T09:00:00+01:00" itemprop="datePublished">
         Mar 22, 2018
      </time>
      </p>
  </header>

  <div class="post-content e-content" itemprop="articleBody">
    <h1 id="brace-yourself-data-selection-industrialization-is-coming">Brace yourself data selection, industrialization is coming!</h1>

<p>Industrialization is one of the most challenging problems for a start-up like ours. In fact, the research world doesn’t have the same priorities concerning time and cost optimization. Whereas industry is limited by these factors. And this matter struck us when we thought about building language models (referred as LM in the following) massively, especially regarding data selection.</p>

<p>Historically, our LMs were crafted one by one with love, with a nice cup of human intervention in between. Meaning that we had to experiment to find the best system empirically. And this is so not compatible with automation.</p>

<p>What is data selection you may ask? However, first thing first.</p>

<h2 id="asr-automatic-speech-recognition-prelude">ASR: Automatic Speech Recognition Prelude</h2>

<p>In ASR, we usually consider building two independent modules which will be mixed together later on. Each module in itself is very dependent of the language we want to recognize.</p>

<ul>
  <li>
    <p>The first one is called acoustic model. It represents the way of speaking. What sounds can be put together to form a word, a sentence. In fact, we represent the language by a serie of phonemes. If we look in <a href="https://en.wiktionary.org/wiki/phoneme">Wiktionary</a> it’s clearer isn’t it? Don’t confuse with syllable through. We can take an example. The word ‘through’ (1 syllable) consists of three sounds, three phonemes: ‘th’ ‘r’ ‘oo’. So the goal is to model the sounds as a sequence of phonemes.</p>
  </li>
  <li>
    <p>The second one is the language model, which we want to build automatically. It models the distribution of words for a given language. Thanks to those probabilities, the LM helps in picking the best correspondence between a sequence of phonemes and the words/sentences.</p>
  </li>
</ul>

<p>In order to build these modules, we need data and the more we have and the more they are relevant, the better! That’s why we need data selection: we need a mean to retrieve relevant data adapted to the context of the recognition. In fact a lawyer and a baker don’t speak the same language: they don’t use the same lexicon. Data selection is picking the good data that match the domain within millions of examples through the usage of various automatic algorithms.</p>

<h2 id="how-we-used-to-do">How we used to do</h2>

<p>As we discussed above, data selection is a very important step while building a system. Like many, we used the <a href="http://www.aclweb.org/anthology/P10-2041">Moore-Lewis</a> method which was also adapted for bilingual use (like translation) by Axelrod et al. in <a href="https://aclanthology.info/pdf/D/D11/D11-1033.pdf">Domain Adaptation via Pseudo In-Domain Data Selection</a>. These are very effective ways to select data using two corpora (in and out domain) by comparing cross-entropies. In-domain meaning that the corpus have specific data, that are relevant with the context, the domain of recognition as explained before with the lawyer/baker thing. Whereas out-of-domain is just a pool of random data, meaning there is relevant and no-relevant data in it! Then about cross-entropy, it’s a measure that help choosing well-matched data for the desired output. Thanks to some relevant segment, we compare each segment in the pool of data to retrieve the closest ones to the initial data.</p>

<p><img src="/assets/DataSelection.jpg" alt="example of data selection" /></p>

<p>So, using the cross-entropy to select, it’s not really scalable because the algorithm can’t decide when to stop on its own and he has an annoying tendency to promote very short to short sentences meaning our corpus isn’t really relevant to conversations. Moreover, something hit us hard. <a href="http://www.aclweb.org/anthology/P10-2041">This paper</a> turned out to be eight this year and we have never looked for another method before. So we asked ourselves: has any new work been done in data selection since this paper? And is there any relevant work ready for a more industrialized turn?</p>

<h2 id="searching-finding">Searching…. Finding!</h2>

<p>After browsing 178 papers quoting the Moore-Lewis one, a title caught our eyes: <a href="https://arxiv.org/pdf/1709.02279.pdf">Cynical Selection of Language Model Training Data</a>. The name was so catchy, we had to explore it. Written by Amittai Axelrod (remember we mentioned him above), we decided to give it a shot <a href="https://github.com/allo-media/cynical-selection">here</a> because the paper was full of good promises … And seemed compatible with industrialization! Unlike the previous methods, the algorithm stops by itself when it has the (supposed) optimal selection, letting us continue our road toward automating.</p>

<h2 id="how-does-it-work-how-did-we-make-it-work">How does it work? How did we make it work?</h2>

<p>The goal is to select data from our out-of-domain corpora that can extend our in-domain data. Suppose you have a small in-domain corpora, which you are a hundred percent positive that is representative. The algorithm will take this corpus and a more generic one, where you don’t know what’s relevant or not. It will then select the sentences that match the specific one using an implementation of the Alexrod’s paper cited above. The script can take arguments which are detailed in the header of the script. It only requires the two corpora to work:</p>

<p><code class="highlighter-rouge">./cynical-selection.py --task inDomainFile.txt --unadapted outDomainFile.txt</code></p>

<p>and returns you a list of sentences along with their scores in a ‘.jaded’ file constructed as follows:</p>

<p><code class="highlighter-rouge"><span class="k">model</span> <span class="n">score</span> <span class="n">sentence</span> <span class="n">score</span> <span class="p">(</span><span class="n">penalty</span> <span class="p">+</span> <span class="n">gain</span><span class="p">)</span> <span class="n">length</span> <span class="n">penalty</span> <span class="n">sentence</span> <span class="n">gain</span> <span class="n">sentence</span> <span class="n">id</span> <span class="p">(</span><span class="k">in</span> <span class="n">the</span> <span class="n">selection</span><span class="p">)</span> <span class="n">sentence</span> <span class="n">id</span> <span class="p">(</span><span class="k">in</span> <span class="n">the</span> <span class="n">unadapted</span> <span class="n">corpora</span><span class="p">)</span> <span class="n">best</span> <span class="n">word</span> <span class="n">word</span> <span class="n">gain</span> <span class="n">sentence</span></code>
for example:</p>

<p><code class="highlighter-rouge">2.659289425334946       2.659289425334946       5.71042701737487        -3.0511375920399235     1       1       vous    -0.12597986190092164    merci à vous tous</code></p>

<p><code class="highlighter-rouge">5.318578850669892       2.659289425334946       5.71042701737487        -3.0511375920399235     2       26978   vous    -0.12597986190092164    et vous avez maintenant</code></p>

<p><code class="highlighter-rouge">7.9778682760048385      2.659289425334946       5.71042701737487        -3.0511375920399235     3       26979   vous    -0.12597986190092164    puisque vous avez des</code></p>

<p>In the end, we didn’t lose any performance using this method, we even gained accuracy most of the time. But the important part is that it allowed us to automatize this treatment, taking us one step closer to industrialization.</p>

<h2 id="to-conclude">To conclude</h2>

<p>This method allows us to focus on other parts of our systems, making us more productive and more serene towards the building of language model. So it’s a success captain!</p>


  </div>

  

  <a class="u-url" href="/r&d/2018/03/22/industrialization-versus-data-selection.html" hidden></a>
</article>

      </div>
    </main><footer class="site-footer">

  <div class="wrapper">

    <h2 class="footer-heading">Allo-Media Technical Blog</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li>
            
              Allo-Media Technical Blog
            
            </li>
            <li>
              <a href="https://www.allo-media.net/about/jobs/">We're hiring!</a>
            </li>
        </ul>
      </div>

      <div class="footer-col footer-col-2">
        <ul class="social-media-list">
          
          <li>
            <a href="https://github.com/allo-media"><span class="icon icon--github"><svg viewBox="0 0 16 16" width="16px" height="16px"><path fill="#828282" d="M7.999,0.431c-4.285,0-7.76,3.474-7.76,7.761 c0,3.428,2.223,6.337,5.307,7.363c0.388,0.071,0.53-0.168,0.53-0.374c0-0.184-0.007-0.672-0.01-1.32 c-2.159,0.469-2.614-1.04-2.614-1.04c-0.353-0.896-0.862-1.135-0.862-1.135c-0.705-0.481,0.053-0.472,0.053-0.472 c0.779,0.055,1.189,0.8,1.189,0.8c0.692,1.186,1.816,0.843,2.258,0.645c0.071-0.502,0.271-0.843,0.493-1.037 C4.86,11.425,3.049,10.76,3.049,7.786c0-0.847,0.302-1.54,0.799-2.082C3.768,5.507,3.501,4.718,3.924,3.65 c0,0,0.652-0.209,2.134,0.796C6.677,4.273,7.34,4.187,8,4.184c0.659,0.003,1.323,0.089,1.943,0.261 c1.482-1.004,2.132-0.796,2.132-0.796c0.423,1.068,0.157,1.857,0.077,2.054c0.497,0.542,0.798,1.235,0.798,2.082 c0,2.981-1.814,3.637-3.543,3.829c0.279,0.24,0.527,0.713,0.527,1.437c0,1.037-0.01,1.874-0.01,2.129 c0,0.208,0.14,0.449,0.534,0.373c3.081-1.028,5.302-3.935,5.302-7.362C15.76,3.906,12.285,0.431,7.999,0.431z"/></svg>
</span><span class="username">allo-media</span></a>

          </li>
          

          
          <li>
            <a href="https://twitter.com/allomedia"><span class="icon icon--twitter"><svg viewBox="0 0 16 16" width="16px" height="16px"><path fill="#828282" d="M15.969,3.058c-0.586,0.26-1.217,0.436-1.878,0.515c0.675-0.405,1.194-1.045,1.438-1.809c-0.632,0.375-1.332,0.647-2.076,0.793c-0.596-0.636-1.446-1.033-2.387-1.033c-1.806,0-3.27,1.464-3.27,3.27 c0,0.256,0.029,0.506,0.085,0.745C5.163,5.404,2.753,4.102,1.14,2.124C0.859,2.607,0.698,3.168,0.698,3.767 c0,1.134,0.577,2.135,1.455,2.722C1.616,6.472,1.112,6.325,0.671,6.08c0,0.014,0,0.027,0,0.041c0,1.584,1.127,2.906,2.623,3.206 C3.02,9.402,2.731,9.442,2.433,9.442c-0.211,0-0.416-0.021-0.615-0.059c0.416,1.299,1.624,2.245,3.055,2.271 c-1.119,0.877-2.529,1.4-4.061,1.4c-0.264,0-0.524-0.015-0.78-0.046c1.447,0.928,3.166,1.469,5.013,1.469 c6.015,0,9.304-4.983,9.304-9.304c0-0.142-0.003-0.283-0.009-0.423C14.976,4.29,15.531,3.714,15.969,3.058z"/></svg>
</span><span class="username">allomedia</span></a>

          </li>
          
        </ul>
      </div>

      <div class="footer-col footer-col-3">
        <p>Random things we&#39;re discovering &amp; learning at Allo-Media.</p>
      </div>
    </div>

  </div>

</footer>
</body>

</html>
